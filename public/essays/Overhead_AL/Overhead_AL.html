<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Active Learning on Overhead Imagery Datasets</title>
    <link rel="stylesheet" href="../../css/main.css">
</head>

<body>
    <div class="container theme-showcase" role="main" id="main">
        <div class='row'>
            <div class="col-md-2">
            </div>
            <div class="col-md-8">
                <div>
                    <div class="page-header" id="active learning overhead">
                        <h2 align="center">Active Learning on Overhead Imagery Datasets</h2>
                    </div>
                    <div align="right">
                    <i>December 29th, 2020</i>
                    </div>
                    <h3 id="introduction">Introduction</h3>
                    <p style="font-size:1.17em">In the last couple of weeks, I&#39;ve been involved in making active learning usable on real-world problems. Ofcourse, that's easier said than done. If you're unfamiliar with active learning, the <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)">wikipedia page</a> is as good an introduction as any. If you want to dig deeper, I recommend this <a href="https://jacobgil.github.io/deeplearning/activelearning#deep-bayesian-active-learning-with-image-data">blogpost</a> by Jacob Gildenblat - that should be enough to give you a solid understanding of what's happening here. If you're more curious, we at <a href="https://alectio.com">Alectio</a> are doing some fantastic things in this space so be sure to follow us on <a href="https://medium.com/alectio">Medium</a>.</p>

                    <p style="font-size:1.17em">The purpose of this short essay is to draw insights from my experiments on two Open Source datasets dealing with ariel imagery. <a href="http://xviewdataset.org">xView</a>, the first one, deals with object detection of more than 60 classes while the second, <a href="https://spacenet.ai/spacenet-buildings-dataset-v1/">SpaceNet Challenge I</a>, is a semantic segmentation task to detect buildings. While SpaceNet is a little bigger in size (4500+ images), xView only consists of 846 images which makes it a more challenging task.</p>

                    <br /> 

                    <figure>
                        <img src='imgs/xView_sample.png' height='250' width='305' style="float:left"/>
                        <img src='imgs/SpaceNet_sample.png' height='250' width='305'style="float:left"/>
                        <figcaption style="color:gray" align="center"><i>Figure 1: A sample of the datasets: xView (left) compared with SpaceNet (right)</i></figcaption>
                    </figure>

                    <h3 id="setup">Setup</h3>
                    <p style="font-size:1.17em">For the sake of simplicity, I've stuck to the following uncertainty-based query strategies: <b>minimum margin</b>, <b>least confidence</b>, <b>highest entropy</b>, and <b>random</b>. The simple goal here is to beat the <b>random</b> query strategy. If you're unfamiliar with the above mentioned methods of choosing data, you can refer to this excellent <a href="http://www.burrsettles.com/pub/settles.activelearning.pdf">literature survery</a> on some of the more common methods. If you're interested in knowing what model-specific hyper-parameters I ended up using, you can find my code <a href="https://github.com/alectio/AlectioSDK/tree/master/examples">here</a>.</p>

                    <div id="list">
                      <ul>
                        <li><p style="font-size:1.17em">For the xView dataset, I used 7 loops with 100 records per loop <i>(700 images)</i>. The remaining 146 images were used for comparing our query strategies. </p></li>
                        <li><p style="font-size:1.17em">For SpaceNet, I used 10 loops with 450 records per loop <i>(4500 / 4858 images)</i>. 1388 images were used for testing our strategies.</p></li>
                      </ul>
                    </div>

                    <p style="font-size:1.17em">There's also two versions to the <b>xView demo</b>. The first one, marked <b>v1</b> from here on out, uses all 60 classes as is. The second version, marked <b>v2</b>, identifies super-classes and reduces the class count to 6. For example in <b>v1</b> there are 5 different kinds of trains and they're treated as different classes but in <b>v2</b> we club them all together. The only reason for this distinction into two versions is that the task is inherently hard and it is in the interest of a fair comparison that the problem be simplified.</p>

                    <p style="font-size:1.17em">Further motivation for taking this step can be gathered from this <a href="https://medium.com/picterra/the-xview-dataset-and-baseline-results-5ab4a1d0f47f">in-depth analysis</a> of the dataset.</p>

                    <h3 id="res">Results</h3>
                    <p style="font-size:1.17em">First, the <b>xView [v1]</b> dataset.</p>

                    <figure>
                        <img src='imgs/xView_v1_results.png' height='250' width='400' style="float:left"/>
                        <img src='imgs/xView_v1_legend.png' height='70' width='230' style="margin-top:20%;margin-bottom:10%"/>
                        <figcaption style="color:gray" align="center"><i>Figure 2: Results on the original xView dataset. The Y axis is the mAP score (x100) while the X axis denotes the number of records selected</i></figcaption>
                    </figure>

                    <br /> 

                    <p style="font-size:1.17em">The result above is quite good. It tells us that <b>high entropy</b> based labeling of 300 records roughly equals <b>random</b> labeling of 700 records. Considering that this is for a dataset with 1 million+ annotations, on average we're looking at savings of more than 470,000 bounding boxes. That's <i>a lot</i> of bounding boxes.</p>

                    <p style="font-size:1.17em">Next up, <b>xView [v2]</b>.</p>

                    <figure>
                        <img src='imgs/xView_v2_results.png' height='250' width='400' style="float:left"/>
                        <img src='imgs/xView_v2_legend.png' height='70' width='230' style="margin-top:20%;margin-bottom:10%"/>
                        <figcaption style="color:gray" align="center"><i>Figure 3: Results on the simplified xView dataset. The Y axis is the mAP score (x100) while the X axis denotes the number of records selected</i></figcaption>
                    </figure>

                    <br /> 

                    <p style="font-size:1.17em">Beware that the legend has changed in <i>Figure 3</i>. What we see here is that <b>high entropy</b> converges after just 200 records and is only surpassed by selecting 500 records using the <b>least confidence</b> query strategy. So even with an easier problem setup, we conclusively surpass random performance. Depending on your budget and desired level of performance, you should work towards identifying a tradeoff in such a scenario. Is it <i>really</i> worth labeling more than 350,000 objects for that 0.009 improvement in the mAP score is the question.</p>

                    <p style="font-size:1.17em">Let's turn our attention to <b>SpaceNet</b> now.</p>

                    <br /> 

                    <figure align="left">
                        <img src='imgs/SpaceNet_pixel_accuracy.png' height='250' width='400'/>
                        <img src='imgs/SpaceNet_mean_iou.png' height='250' width='400'/>
                        <img src='imgs/SpaceNet_freqw_acc.png' height='250' width='400'/>
                        <figcaption style="color:gray" align="center"><i>Figure 3: Results on the simplified SpaceNet dataset. The Y axis is the metric score while the X axis denotes the number of loops</i></figcaption>
                    </figure>

                    <br /> 
                   
                   <p style="font-size:1.17em">The active-learning based improvements on this new dataset are a little harder to see and that's because the regular training loop converges very quickly (even without any pretraining). The first metric noted above, <b>pixel accuracy</b> is a little irrelevant in this dataset that contains only two classes: <i>building</i> and <i>background</i>. Since most pixels are background, a model that just outputs zero tensors also achieves a score greater than 0.9. Looking ahead at <b>mean iou</b>, things get a little more interesting. Here we see that the best performing <b>random</b> model at loop 8 performs a little worse than <b>least margin</b> at loop 3. Hence, going by this techique, we would save labeling on 2250 images.</p>

                    <p style="font-size:1.17em">Furthermore, <b>least confident</b> in the above diagram consistently beats <b>random</b> until loop 7 in all three metrics. Loop 2 with the former beats the latter in all three cases until loop 7. That's again 2250 records that we would save on labeling. To understand how much <b style="color:green">$$$</b> we can save, we can take a look at this <a href="https://cloud.google.com/ai-platform/data-labeling/pricing">chart</a> by Google cloud.</p>

                    <figure align="left">
                        <img src='imgs/labeling_cost.png' height='450' width='660'/>                        
                        <figcaption style="color:gray" align="center"><i>Figure 4: Google Cloud labeling pricing. It's among the lowest in the industry.</i></figcaption>
                    </figure>
                </div>

                <h3 id="conc">Conclusion</h3>
                <p style="font-size:1.17em">Using a larger dataset will, almost always, result in a performance improvement but the cost can be particulary large when the labeling is to be done by an expert. Making active learning a standard practice in the industry would lead to faster deployment of new datasets along with incredible cost savings even in challenging problem domains (as illustrated by this post).</p>

                <p style="font-size:1.17em">Thank you for reading! If you have any further questions regarding this post, feel free to contact me at prateekmalhotra [at] ucla [dot] edu.</p>

            </div>
                <div class="col-md-2">
            </div>
        </div>
    </div>
</body>
</html>
